{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create VOLOv7 for TF-Lite\n",
    "\n",
    "You  Only  Look  Once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know the drill: Create a virtual environment and install the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Project imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import onnxruntime as ort\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting YOLO\n",
    "\n",
    "We'll start off by cloning YOLOv7 from the official repo.  We'll also get the latest weights.\n",
    "\n",
    "YOLOv7 is written using pytorch.  Thankfully, those good folks at YOLO gave us an export script to translate their inference model to ONNX format, so we'll do that now.  All this happens in the next cell.  Sit tight.\n",
    "\n",
    "### `export.py` Parameter explanation\n",
    "\n",
    "- **grid**:  The grid parameter is an option allowing the export of the detection layer grid.\n",
    "- **end2end**:  It is the option that allows the export of end-to-end ONNX graph which does both bounding box prediction and NMS.\n",
    "- **simplify**:  It is the option by which we can select whether we want to simplify the ONNX graph using reparameterization.\n",
    "- **topk-all**:  It's the option to select the top k object per image using IOU and confidence threshold.\n",
    "- **iou-thres**:  It is the option to set the IOU threshold for NMS.\n",
    "- **conf-thres**:  It is the option to select the confidence threshold score.\n",
    "- **img_size and max-wh**:  These parameters are related to the size of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/WongKinYiu/yolov7.git\n",
    "%cd yolov7\n",
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n",
    "!python3 export.py --weights yolov7.pt --grid --end2end --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640 --max-wh 640\n",
    "%cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tada!  We have YOLO, and it's in ONNX format for us.  Here's what we get out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order matters for cool freebies.\n",
    "names = ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "         'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "         'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "         'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "         'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "         'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "         'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "         'hair drier', 'toothbrush')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(input_img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scale_up=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "\n",
    "    old_shape = input_img.shape[:2] # current shape [height, width]\n",
    "\n",
    "    # if it's just an int, assume it's a square\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    ratio = min(new_shape[0] / old_shape[0], new_shape[1] / old_shape[1])\n",
    "    if not scale_up:  # only scale down, do not scale up (for better val mAP!)\n",
    "        ratio = min(ratio, 1.0)\n",
    "\n",
    "    # Compute padding, HxW\n",
    "    hw_padding: tuple = (int(round(old_shape[1] * ratio)), int(round(old_shape[0] * ratio)))\n",
    "    delta_w, delta_h = new_shape[1] - hw_padding[0], new_shape[0] - hw_padding[1]\n",
    "\n",
    "    # minimum rectangle\n",
    "    if auto:\n",
    "        delta_h, delta_w = np.mod(delta_h, stride), np.mod(delta_w, stride)\n",
    "\n",
    "    # divide padding into 2 sides\n",
    "    delta_w /= 2\n",
    "    delta_h /= 2\n",
    "\n",
    "    # resize\n",
    "    if old_shape[::-1] != hw_padding:\n",
    "        output_img = cv2.resize(input_img, hw_padding, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(delta_h - 0.1)), int(round(delta_h + 0.1))\n",
    "    left, right = int(round(delta_w - 0.1)), int(round(delta_w + 0.1))\n",
    "    output_img = cv2.copyMakeBorder(output_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return output_img, ratio, (delta_w, delta_h)\n",
    "\n",
    "# Notice we are using enumerate(output_data)\n",
    "def display(frame, scale_deltas, ratio, colors, outputs, names=names):\n",
    "\n",
    "    for (_, x0, y0, x1, y1, cls_id, score) in outputs:\n",
    "        bound_box = np.array([x0, y0, x1, y1])\n",
    "        bound_box -= np.array(scale_deltas * 2)\n",
    "        bound_box /= ratio\n",
    "        bound_box = bound_box.round().astype(np.int32).tolist()\n",
    "        cls_id = int(cls_id)\n",
    "        score = round(float(score), 3)\n",
    "        score = round(score * 100)\n",
    "        name = names[cls_id]\n",
    "        color = colors[name]\n",
    "        label = f'{name} {score}%'\n",
    "        cv2.rectangle(img=frame,\n",
    "                      pt1=bound_box[:2],\n",
    "                      pt2=bound_box[2:],\n",
    "                      color=color,\n",
    "                      thickness=2)\n",
    "\n",
    "        # We do it backwards. get size/coordinates before *actually* create the text\n",
    "        (w, h), _ = cv2.getTextSize(text=label,\n",
    "                                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                    fontScale=0.6,\n",
    "                                    thickness=1)\n",
    "        # class label\n",
    "        cv2.rectangle(img=frame,\n",
    "                      pt1=(bound_box[0]-1, bound_box[1]),\n",
    "                      pt2=(bound_box[0]+w, bound_box[1]-h-6),\n",
    "                      color=color,\n",
    "                      thickness=-1)\n",
    "\n",
    "        cv2.putText(img=frame,\n",
    "            text=label,\n",
    "            org=(bound_box[0]+5, bound_box[1] - 5),\n",
    "            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=.5,\n",
    "            color=[255, 255, 255],\n",
    "            thickness=1,\n",
    "            lineType=cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('VIDEO', frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False\n",
    "os_path = os.getcwd()\n",
    "\n",
    "#Loading image(s) for testing.\n",
    "img = cv2.imread(f'{os_path}/content/6.jpg')\n",
    "\n",
    "#Create some random colors for bounding box visualizations.\n",
    "colors = {name:[random.randint(0, 255) for _ in range(3)] for i,name in enumerate(names)}\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Pre-processing the image.\n",
    "image = img.copy()\n",
    "image, ratio, scale_deltas = letterbox(image, auto=False)\n",
    "image = image.transpose((2, 0, 1))\n",
    "image = np.expand_dims(image, 0)\n",
    "image = np.ascontiguousarray(image)\n",
    "\n",
    "# good ol' normalization\n",
    "np_img = image.astype(np.float32)\n",
    "np_img /= 255\n",
    "print(f'shape: {np_img.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference with ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if cuda else ['CPUExecutionProvider']\n",
    "session = ort.InferenceSession(f'{os_path}/yolov7/yolov7.onnx', providers=providers)\n",
    "\n",
    "#Getting onnx graph input and output names.\n",
    "output_names = [i.name for i in session.get_outputs()]\n",
    "input_names = [i.name for i in session.get_inputs()]\n",
    "input_feed = {input_names[0]: np_img}\n",
    "onnx_outputs = session.run(output_names=output_names, input_feed=input_feed)[0]\n",
    "ori_img = [img.copy()]\n",
    "\n",
    "#Visualizing bounding box prediction.\n",
    "display(frame=image, scale_deltas=scale_deltas, ratio=ratio, colors=colors, outputs=onnx_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes the command needs to be ran in your shell, dunno why ATM.\n",
    "!onnx-tf convert -i yolov7/yolov7.onnx -o ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model to TF-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(os_path)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(f'{os_path}/yolov7_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"yolov7_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_shape = input_details[0]['shape']\n",
    "interpreter.set_tensor(input_details[0]['index'], np_img)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "tfl_out = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(f'shape: {tfl_out.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference on TF-Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(frame=image, scale_deltas=scale_deltas, ratio=ratio, colors=colors, outputs=tfl_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a345419ffbc3fde4518709094678c14442b9181ab8ec889d570b72d955b790f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
